{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1980a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Functions that generate simulated data, and split the simulated data into train, cal and test.\n",
    "import numpy as np\n",
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "\n",
    "def find_beta(d,R_squared,error):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    d: The dimension of X, e.g., d=1000\n",
    "    R_squared: \n",
    "    error: homo or hetero\n",
    "    \n",
    "    output:\n",
    "    beta: a scaler\n",
    "    \"\"\"\n",
    "    assert error in ['homo','hetero'], 'error must be homo or hetero!'\n",
    "    total_sum=0\n",
    "    for j in range(2, d+1):\n",
    "        term = 1 / (j**2)\n",
    "        total_sum += term\n",
    "    \n",
    "    beta=symbols('beta')\n",
    "    if error=='homo':\n",
    "        equation=Eq(total_sum*beta**2-R_squared*(total_sum*beta**2+1),0)\n",
    "        solutions=solve(equation,beta)\n",
    "        beta_hat=[sol.evalf() for sol in solutions if sol > 0]\n",
    "    else:\n",
    "        equation=Eq(total_sum*beta**2-R_squared*(total_sum*beta**2+35),0)\n",
    "        solutions=solve(equation,beta)\n",
    "        beta_hat=[sol.evalf() for sol in solutions if sol > 0]\n",
    "    \n",
    "    return beta_hat\n",
    "\n",
    "def generate_data(n,d,R_squared,error,mean):\n",
    "    \"\"\"\n",
    "    n: sample size\n",
    "    d: dimensionality of X, e.g, d=1000\n",
    "    R_squared: \n",
    "    error: homo or hetero\n",
    "    mean:\n",
    "    \n",
    "    \"\"\"\n",
    "    assert error in ['homo','hetero'], 'error must be homo or hetero!'\n",
    "    #mean=np.full(d,mean)\n",
    "    #cov=np.eye(d)\n",
    "    #X=np.random.multivariate_normal(mean,cov,size=n)\n",
    "    X=np.random.normal(mean,1,(n,d))\n",
    "    X[:,0]=1\n",
    "\n",
    "    beta_hat=find_beta(d,R_squared,error)\n",
    "    coef=[1/(j+1) for j in range(d)]\n",
    "    coef=np.array(coef)\n",
    "    \n",
    "    coef=coef*beta_hat\n",
    "    Y_reg=X.dot(coef)\n",
    "    \n",
    "    if error=='homo':\n",
    "        eps=np.random.normal(loc=0,scale=1,size=n)\n",
    "    else:\n",
    "        eps0=np.random.normal(loc=0,scale=1,size=n)\n",
    "        X_sub=X[:,1:6]\n",
    "        hetero_x_squared=np.sum(X_sub**2,axis=1)\n",
    "        eps=hetero_x_squared*eps0\n",
    "    \n",
    "    Y=Y_reg+eps\n",
    "    \n",
    "    data={'X':X,'Y':Y}\n",
    "    return data\n",
    "        \n",
    "def split_data(data,n0,n1,n,d_set='train'):\n",
    "    \"\"\"\n",
    "    n0:train data\n",
    "    n1: calibration data\n",
    "    n:test data\n",
    "    \"\"\"\n",
    "    assert d_set in ['train', 'cal','test'], 'd_set must be train, cal, or test.'\n",
    "    \n",
    "    if d_set=='train':\n",
    "        X=data['X'][:n0]\n",
    "        Y=data['Y'][:n0]\n",
    "    elif d_set=='cal':\n",
    "        X=data['X'][n0:(n0+n1)]\n",
    "        Y=data['Y'][n0:(n0+n1)]\n",
    "    else:\n",
    "        X=data['X'][n0+n1:]\n",
    "        Y=data['Y'][n0+n1:]\n",
    "        \n",
    "    data_split={'X':X,'Y':Y}\n",
    "    return data_split\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfa851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "def fit_candidate_model(X_train,Y_train,M):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    X_train: n*d dimensiolal \n",
    "    Y_train: n dim\n",
    "    M: the number of candidate models\n",
    "    \n",
    "    output:\n",
    "    M candidate trained models\n",
    "    \"\"\"\n",
    "    candidate_models=[]\n",
    "    for i in range(M):\n",
    "        X_train_candidate=copy.deepcopy(X_train[:,1:i+2])\n",
    "        trained_model = LinearRegression(n_jobs=2).fit(X_train_candidate,Y_train)\n",
    "        candidate_models.append(trained_model)\n",
    "    \n",
    "    return candidate_models\n",
    "\n",
    "def jackknife_prediction(X_train,Y_train,M):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    X_train: n*d dimensiolal array\n",
    "    Y_train: n dim array\n",
    "    M: the number of candidate models\n",
    "    \n",
    "    output:\n",
    "    prediction_jackknife: n_train*M dimensional array\n",
    "    \"\"\"\n",
    "    n_train=len(Y_train)\n",
    "    prediction_jackknife=np.zeros((M,n_train))\n",
    "    \n",
    "    for i in range(M):\n",
    "        X_train_candidate=copy.deepcopy(X_train[:,1:i+2])\n",
    "        for j in range(n_train):\n",
    "            X_train_jackknife=np.delete(X_train_candidate,j,axis=0)\n",
    "            Y_train_jackknife=np.delete(Y_train,j)\n",
    "            trained_model = LinearRegression(n_jobs=2).fit(X_train_jackknife,Y_train_jackknife)\n",
    "            \n",
    "            X_test_jackknife=X_train_candidate[j,:].reshape(1, -1)\n",
    "            prediction_jackknife[i,j]=trained_model.predict(X_test_jackknife)\n",
    "    \n",
    "    prediction_jackknife=prediction_jackknife.T\n",
    "    return prediction_jackknife\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600f6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Function that find the optimal weights\n",
    "from qpsolvers import solve_qp\n",
    "import scipy.sparse as sp\n",
    "def weights_criterion_jackknife(Y_train,prediction_jackknife,kernel_weights,candidate_interval_lengths,penalty_coef,criterion):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    Y_train: n dimensional array\n",
    "    prediction_jackknife: n*M dim\n",
    "    kernel_weights: n dimensioal array\n",
    "    candidate_interval_lengths: M dimensional array\n",
    "    penalty_coef: i.e.,lambda, a scalar\n",
    "    criterion: ordinary, weighted, or weighted_plus_penalty\n",
    "    \n",
    "    output:\n",
    "    optimal_weights: M dimensional array\n",
    "    \n",
    "    \"\"\"\n",
    "    assert criterion in ['ordinary','weighted','weighted_plus_penalty'], 'criterion must be ordinary, weighted, or weighted_plus_penalty!'\n",
    "    \n",
    "    n_train=prediction_jackknife.shape[0]\n",
    "    M=prediction_jackknife.shape[1]\n",
    "    A=np.ones(M).reshape((M,))\n",
    "    A=sp.csc_matrix(A)\n",
    "    b = np.array([1], dtype=np.double)\n",
    "    lb=np.zeros(M).reshape((M,))\n",
    "    ub=np.ones(M).reshape((M,))\n",
    "    \n",
    "    if criterion=='ordinary':\n",
    "        P_kernel=np.eye(n_train)\n",
    "        penalty_coef=0\n",
    "        L=np.zeros((M,M))\n",
    "    elif criterion=='weighted':\n",
    "        assert kernel_weights is not None, 'kernel_weights must be specified!'\n",
    "        P_kernel=np.diag(kernel_weights)\n",
    "        penalty_coef=0\n",
    "        L=np.zeros((M,M))\n",
    "    else:\n",
    "        assert kernel_weights is not None, 'kernel_weights must be specified!'\n",
    "        assert penalty_coef is not None, 'penalty_coef must be specified!'\n",
    "        assert candidate_interval_lengths is not None, 'candidate_interval_lengths must be specified!'\n",
    "        P_kernel=np.diag(kernel_weights)\n",
    "        penalty_coef=penalty_coef\n",
    "        L=np.diag(candidate_interval_lengths)\n",
    "    \n",
    "    P=2*(prediction_jackknife.T.dot(P_kernel).dot(prediction_jackknife)+penalty_coef*L.T.dot(L))\n",
    "    P=sp.csc_matrix(P)\n",
    "    q=-2*prediction_jackknife.T.dot(P_kernel).dot(Y_train)\n",
    "    optimal_weights=solve_qp(P=P, q=q, A=A, b=b,lb=lb,ub=ub,solver=\"scs\")\n",
    "    \n",
    "    return optimal_weights\n",
    "    \n",
    "\n",
    "def find_kernel_weights(X,x_datapoint,h):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    X: n*d dimensional array\n",
    "    x_datapoint: a single data point, d dimensional array\n",
    "    h: bandwidth\n",
    "    \n",
    "    output:\n",
    "    kernel_weights: n dimensional array\n",
    "    \"\"\"\n",
    "    n_X=X.shape[0]\n",
    "    d=X.shape[1]\n",
    "    distances=np.zeros(n_X)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    X=X[:,0:6]\n",
    "    x_datapoint=x_datapoint[0:6]\n",
    "    \n",
    "    coef=[1/(j+1) for j in range(d)]\n",
    "    coef=np.array(coef)\n",
    "    X=X*coef\n",
    "    x_datapoint=x_datapoint*coef  \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    for i in range(n_X):\n",
    "        distances[i] = np.linalg.norm(x_datapoint - X[i,],ord=2)**2\n",
    "    \n",
    "    kernel=np.exp(-distances/(2*h**2))\n",
    "    kernel_weights=kernel/np.sum(kernel)\n",
    "    \n",
    "    return kernel_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6015ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_corrected(x,alpha):\n",
    "    n_x=len(x)\n",
    "    if(1-alpha)*(1+1/n_x)>1:\n",
    "        return np.inf\n",
    "    else:\n",
    "        return np.quantile(x,(1-alpha)*(1+1/n_x))\n",
    "\n",
    "def weighted_quantile(scores,weights,alpha):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    scores: n dimensional array\n",
    "    weights: n dimensional array. Note that scores and weights are one-to-one.\n",
    "    \n",
    "    output:\n",
    "    quantile: a scalar\n",
    "    \n",
    "    \"\"\"\n",
    "    scores_sorted=np.sort(scores)\n",
    "    indices=np.argsort(scores)\n",
    "    weights_sorted=weights[indices]\n",
    "    cumulative_weights=np.cumsum(weights_sorted)\n",
    "    index = np.where(cumulative_weights >= 1-alpha)[0][0]\n",
    "    quantile=scores_sorted[index]\n",
    "    \n",
    "    return quantile\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f2d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_basemodel(M_candidate_models,optimal_weights,X_test):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    M_candidate_models: obtained from the function \"fit_candidate_model\"\n",
    "    optimal_weights: M dimensional array\n",
    "    X_test: n*d dimensianal array\n",
    "    \n",
    "    output:\n",
    "    predictions: n dimensional array\n",
    "    \n",
    "    \"\"\"\n",
    "    M=len(optimal_weights)\n",
    "    #check if X_test is a single datapoint\n",
    "    if X_test.ndim==1:\n",
    "        predictions_M_candidate=np.zeros((M,1))\n",
    "        for i in range(M):\n",
    "            X_test_candidate=copy.deepcopy(X_test[1:i+2]).reshape(1,-1)\n",
    "            predictions_M_candidate[i,]=M_candidate_models[i].predict(X_test_candidate)\n",
    "            \n",
    "    else:\n",
    "        n_test=X_test.shape[0]\n",
    "        predictions_M_candidate=np.zeros((M,n_test))\n",
    "        for i in range(M):\n",
    "            X_test_candidate=copy.deepcopy(X_test[:,1:i+2])\n",
    "            predictions_M_candidate[i,]=M_candidate_models[i].predict(X_test_candidate)\n",
    "        \n",
    "    predictions=predictions_M_candidate.T.dot(optimal_weights)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc51e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Functions that compute point prediction error, interval coveerage and length.\n",
    "def compute_PP_metric(predictions,Y_test):\n",
    "    \"\"\"\n",
    "    Compute the prediction error.\n",
    "    \n",
    "    \"\"\"\n",
    "    error=np.sum(np.abs(predictions-Y_test))/len(Y_test)\n",
    "    return error\n",
    "\n",
    "def compute_PI_metrics(interval_predictions,Y_test):\n",
    "    \"\"\"\n",
    "    Compute interval coverage and length.\n",
    "    \"\"\"\n",
    "    contains = (Y_test <= interval_predictions['y_sup']) & (Y_test >= interval_predictions['y_inf'])\n",
    "    lengths = interval_predictions['y_sup'] - interval_predictions['y_inf']\n",
    "\n",
    "    coverage=np.mean(contains)\n",
    "    len=np.mean(lengths)\n",
    "    metrics={'cov':coverage,\n",
    "             'len':len}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d8dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function that implement SCP_MA\n",
    "def SCP_MA(M_candidate_models,prediction_jackknife,Y_train,X_cal,Y_cal,X_test,alpha):\n",
    "    \"\"\"\n",
    "    Split conformal prediction for model averaging\n",
    "    input: \n",
    "    M_candidate_model:\n",
    "    prediction_jackknife:\n",
    "    Y_train:\n",
    "    X_cal:\n",
    "    Y_cal:\n",
    "    X_test:\n",
    "    alpha:\n",
    "    \n",
    "    output:\n",
    "    interval_predictions:\n",
    "    \n",
    "    \"\"\"\n",
    "    Y_train=np.array(Y_train,dtype=np.double)\n",
    "    JMA_weights=weights_criterion_jackknife(Y_train=Y_train,prediction_jackknife=prediction_jackknife,\n",
    "                                            kernel_weights=None,candidate_interval_lengths=None,penalty_coef=None,criterion='ordinary')\n",
    "    pred_cal=predict_basemodel(M_candidate_models,JMA_weights,X_cal)\n",
    "    scores=np.abs(pred_cal-Y_cal)\n",
    "    q_scores=quantile_corrected(scores, alpha)\n",
    "    \n",
    "    pred_test=predict_basemodel(M_candidate_models,JMA_weights,X_test)\n",
    "    interval_predictions={'y_inf':pred_test-q_scores,\n",
    "                          'y_sup':pred_test+q_scores}\n",
    "    \n",
    "    return interval_predictions\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4ab5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Functions that implement SLCP_MA\n",
    "def compute_candidate_model_lengths(M_candidate_models,X_train,Y_train,X_cal,Y_cal,x_testpoint,h,alpha):\n",
    "    \"\"\"\n",
    "    Compute the prediction length of M candidate models\n",
    "    input:\n",
    "    M_candidate_models: \n",
    "    X_train:\n",
    "    Y_train:\n",
    "    X_cal:\n",
    "    Y_cal:\n",
    "    x_testpoint: a single datapoint\n",
    "    h:\n",
    "    alpha:\n",
    "    \n",
    "    output:\n",
    "    candidate_model_lengths: M dim\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    M=len(M_candidate_models)\n",
    "    n_train=len(Y_train)\n",
    "    n_cal=len(Y_cal)\n",
    "    candidate_model_lengths=np.zeros(M)\n",
    "    \n",
    "    kernel_weights_matrix=np.zeros((n_cal+1,n_train)) #ecah row corresponding to a specific kernel weights\n",
    "    for i in range(n_cal):\n",
    "        kernel_weights_matrix[i,]=find_kernel_weights(X_train,X_cal[i,:],h)\n",
    "    \n",
    "    kernel_weights_matrix[-1,:]=find_kernel_weights(X_train,x_testpoint,h)\n",
    "    \n",
    "    for i in range(M):\n",
    "        X_train_candidate=copy.deepcopy(X_train[:,1:i+2])\n",
    "        X_cal_candidate=copy.deepcopy(X_cal[:,1:i+2])\n",
    "        \n",
    "        pred_train_candidate=M_candidate_models[i].predict(X_train_candidate)\n",
    "        pred_cal_candidate=M_candidate_models[i].predict(X_cal_candidate)\n",
    "        \n",
    "        scores_train=np.abs(pred_train_candidate-Y_train)\n",
    "        scores_cal=np.abs(pred_cal_candidate-Y_cal)\n",
    "        \n",
    "        ##quantile_weighted:each element corresponding to Q(1-alpha,F_{i}^{h}), for i in [1,...,n+1].\n",
    "        quantile_weighted=np.zeros(n_cal+1) \n",
    "        for j in range(n_cal+1):\n",
    "            quantile_weighted[j]=weighted_quantile(scores_train,kernel_weights_matrix[j,:],alpha)\n",
    "        \n",
    "        V_scores=scores_cal-quantile_weighted[:-1]\n",
    "        d1=quantile_weighted[-1]+quantile_corrected(V_scores,alpha)\n",
    "        candidate_model_lengths[i]=2*d1\n",
    "    \n",
    "    return candidate_model_lengths\n",
    "            \n",
    "def SLCP_MA(M_candidate_models,prediction_jackknife,X_train,Y_train,X_cal,Y_cal,X_test,h,penalty_coef,alpha):\n",
    "    \"\"\"\n",
    "    Split localized conformal prediction for model averaging\n",
    "    input:\n",
    "    \n",
    "    output:\n",
    "    interval_predictions:\n",
    "    \n",
    "    \"\"\"\n",
    "    Y_train=np.array(Y_train,dtype=np.double)\n",
    "    n_train=X_train.shape[0]\n",
    "    n_cal=X_cal.shape[0]\n",
    "    n_test=X_test.shape[0]\n",
    "    \n",
    "    kernel_weights_matrix=np.zeros((n_cal+1,n_train))\n",
    "    \n",
    "    for i in range(n_cal):\n",
    "        kernel_weights_matrix[i,]=find_kernel_weights(X_train,X_cal[i,],h)\n",
    "    \n",
    "    y_inf=np.zeros(n_test)\n",
    "    y_sup=np.zeros(n_test)\n",
    "    for i in range(n_test):\n",
    "        x_testpoint=X_test[i,]\n",
    "        kernel_weights=find_kernel_weights(X_train,x_testpoint,h)\n",
    "        kernel_weights_matrix[-1,:]=kernel_weights\n",
    "        \n",
    "        candidate_interval_lengths=compute_candidate_model_lengths(M_candidate_models,X_train,Y_train,X_cal,Y_cal,x_testpoint,h,alpha)\n",
    "        optimal_weights=weights_criterion_jackknife(Y_train,prediction_jackknife,kernel_weights,candidate_interval_lengths,penalty_coef,criterion='weighted_plus_penalty')\n",
    "        \n",
    "        pred_train=predict_basemodel(M_candidate_models,optimal_weights,X_train)\n",
    "        pred_cal=predict_basemodel(M_candidate_models,optimal_weights,X_cal)\n",
    "        \n",
    "        scores_train=np.abs(pred_train-Y_train)\n",
    "        scores_cal=np.abs(pred_cal-Y_cal)\n",
    "        \n",
    "        quantile_weighted=np.zeros(n_cal+1)\n",
    "        for j in range(n_cal+1):\n",
    "            quantile_weighted[j]=weighted_quantile(scores_train,kernel_weights_matrix[j,:],alpha)\n",
    "        \n",
    "        V_scores=scores_cal-quantile_weighted[:-1]\n",
    "        d1=quantile_weighted[-1]+quantile_corrected(V_scores,alpha)\n",
    "        \n",
    "        pred_testpoint=predict_basemodel(M_candidate_models,optimal_weights,x_testpoint)\n",
    "        y_inf[i]=pred_testpoint-d1\n",
    "        y_sup[i]=pred_testpoint+d1\n",
    "    \n",
    "    interval_predictions={'y_inf':y_inf,\n",
    "                          'y_sup':y_sup}\n",
    "    \n",
    "    return interval_predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5adfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc754e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01af1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6af028",
   "metadata": {},
   "source": [
    "# Simulation studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689464b3",
   "metadata": {},
   "source": [
    "## Compare prediction errors of two model averaging methods, and compare coverage and interval length of SCP_MA and SLCP_MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "002d3986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "###setting\n",
    "n0=50    #training set\n",
    "n1=50    #calibration set\n",
    "n=100   #test data\n",
    "M=int(3*n0**(1/3))\n",
    "d=50 #The dimensionality of X\n",
    "R_squared=0.9  ##ranging from 0.1 to 0.9\n",
    "error='homo' #homo or hetero\n",
    "h=5 # The bandwidth of Gaussian kernel\n",
    "penalty_coef=1.0  #penalty term\n",
    "alpha=0.1\n",
    "replication=1\n",
    "\n",
    "mean0=0\n",
    "mean1=0.3\n",
    "\n",
    "\n",
    "JMA_risk=np.zeros(replication)\n",
    "weighted_JMA_risk=np.zeros(replication)\n",
    "\n",
    "SCP_MA_cov=np.zeros(replication)\n",
    "SCP_MA_len=np.zeros(replication)\n",
    "SLCP_MA_cov=np.zeros(replication)\n",
    "SLCP_MA_len=np.zeros(replication)\n",
    "\n",
    "for k in range(replication):\n",
    "    df=generate_data(n0+n1+n,d,R_squared,error,mean=mean0)\n",
    "    df1=split_data(df,n0,n1,n,d_set='train')\n",
    "    df2=split_data(df,n0,n1,n,d_set='cal')\n",
    "    \n",
    "    #df=generate_data(n0+n1+n,d,R_squared,error,mean=mean1)\n",
    "    df3=split_data(df,n0,n1,n,d_set='test') \n",
    "    \n",
    "    #train M candidate models\n",
    "    M_candidate_models=fit_candidate_model(df1['X'],df1['Y'],M)\n",
    "    \n",
    "    prediction_jackknife=jackknife_prediction(df1['X'],df1['Y'],M)\n",
    "    \n",
    "    Y_train=np.array(df1['Y'],dtype=np.double)\n",
    "    \n",
    "    ##################################First: compare their test errors--------------------------------------------\n",
    "    ##JMA\n",
    "    JMA_weights=weights_criterion_jackknife(Y_train=Y_train,prediction_jackknife=prediction_jackknife,\n",
    "                                            kernel_weights=None,candidate_interval_lengths=None,penalty_coef=None,criterion='ordinary')\n",
    "    JMA_predictions=predict_basemodel(M_candidate_models,JMA_weights,df3['X'])\n",
    "    \n",
    "    ##weighted_JMA\n",
    "    weighted_JMA_predictions=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        kernel_weights=find_kernel_weights(X=df1['X'],x_datapoint=df3['X'][i,],h=h)\n",
    "        weighted_JMA_weights=weights_criterion_jackknife(Y_train=Y_train,prediction_jackknife=prediction_jackknife,\n",
    "                                                         kernel_weights=kernel_weights,candidate_interval_lengths=None,penalty_coef=None,criterion='weighted')\n",
    "        weighted_JMA_predictions[i]=predict_basemodel(M_candidate_models,weighted_JMA_weights,df3['X'][i,:])   \n",
    "    \n",
    "    ###compute their errors\n",
    "    JMA_risk[k]=compute_PP_metric(predictions=JMA_predictions,Y_test=df3['Y'])\n",
    "    weighted_JMA_risk[k]=compute_PP_metric(predictions=weighted_JMA_predictions,Y_test=df3['Y'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##################################Second: compare their coverage and interval length--------------------------------------------\n",
    "    SCP_MA_pred_intervals=SCP_MA(M_candidate_models,prediction_jackknife,df2['Y'],df2['X'],df2['Y'],df3['X'],alpha)\n",
    "    \n",
    "    SLCP_MA_pred_intervals=SLCP_MA(M_candidate_models,prediction_jackknife,df1['X'],df1['Y'],df2['X'],df2['Y'],df3['X'],h,penalty_coef,alpha)\n",
    "    \n",
    "    \n",
    "    #####\n",
    "    SCP_MA_metrics=compute_PI_metrics(SCP_MA_pred_intervals,df3['Y'])\n",
    "    SLCP_MA_metrics=compute_PI_metrics(SLCP_MA_pred_intervals,df3['Y'])\n",
    "    \n",
    "    ###\n",
    "    SCP_MA_cov[k]=SCP_MA_metrics['cov']\n",
    "    SCP_MA_len[k]=SCP_MA_metrics['len']\n",
    "    SLCP_MA_cov[k]=SLCP_MA_metrics['cov']\n",
    "    SLCP_MA_len[k]=SLCP_MA_metrics['len']\n",
    "    \n",
    "    \n",
    "    print(k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80be983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"JMA and weighted_JMA prediction error: : \", round(np.mean(JMA_risk),3),round(np.mean(weighted_JMA_risk),3))\n",
    "#print(\"SCP_MA: coverage and length: \", round(np.mean(SCP_MA_cov),3),round(np.mean(SCP_MA_len),3))\n",
    "#print(\"SLCP_MA: coverage and length: \", round(np.mean(SLCP_MA_cov),3),round(np.mean(SLCP_MA_len),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d8d5816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JMA and weighted_JMA prediction error: :  1.42 1.431\n",
      "coverage:  0.87 0.76\n",
      "length:  8.615 4.163\n"
     ]
    }
   ],
   "source": [
    "print(\"JMA and weighted_JMA prediction error: : \", round(np.mean(JMA_risk),3),round(np.mean(weighted_JMA_risk),3))\n",
    "print(\"coverage: \", round(np.mean(SCP_MA_cov),3),round(np.mean(SLCP_MA_cov),3))\n",
    "print(\"length: \", round(np.mean(SCP_MA_len),3),round(np.mean(SLCP_MA_len),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb3e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6200d8c2",
   "metadata": {},
   "source": [
    "# Run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0181ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "R_squared_vec=np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])  ##ranging from 0.1 to 0.9\n",
    "num_R_squared=len(R_squared_vec)\n",
    "\n",
    "JMA_risk_vec=np.zeros(num_R_squared)\n",
    "weighted_JMA_risk_vec=np.zeros(num_R_squared)\n",
    "SCP_cov_vec=np.zeros(num_R_squared)\n",
    "SCP_len_vec=np.zeros(num_R_squared)\n",
    "SLCP_cov_vec=np.zeros(num_R_squared)\n",
    "SLCP_len_vec=np.zeros(num_R_squared)\n",
    "\n",
    "\n",
    "for t in range(num_R_squared):\n",
    "    n0=50*3    #training set\n",
    "    n1=50*3    #calibration set\n",
    "    n=100   #test data\n",
    "    M=int(3*n0**(1/3))\n",
    "    d=50 #The dimension of X\n",
    "\n",
    "    error='homo' #homo or hetero\n",
    "    h=1 # The bandwidth of Gaussian kernel\n",
    "    penalty_coef=1  #penalty term\n",
    "    alpha=0.1\n",
    "    replication=15\n",
    "    R_squared=R_squared_vec[t]  ##ranging from 0.1 to 0.9\n",
    "    mean0=0\n",
    "    #mean1=0.3\n",
    "\n",
    "\n",
    "    JMA_risk=np.zeros(replication)\n",
    "    weighted_JMA_risk=np.zeros(replication)\n",
    "\n",
    "    SCP_MA_cov=np.zeros(replication)\n",
    "    SCP_MA_len=np.zeros(replication)\n",
    "    SLCP_MA_cov=np.zeros(replication)\n",
    "    SLCP_MA_len=np.zeros(replication)\n",
    "\n",
    "    for k in range(replication):\n",
    "        df=generate_data(n0+n1+n,d,R_squared,error,mean=mean0)\n",
    "        df1=split_data(df,n0,n1,n,d_set='train')\n",
    "        df2=split_data(df,n0,n1,n,d_set='cal')\n",
    "\n",
    "        #df=generate_data(n0+n1+n,d,R_squared,error,mean=mean1)\n",
    "        df3=split_data(df,n0,n1,n,d_set='test') \n",
    "\n",
    "        #train M candidate models\n",
    "        M_candidate_models=fit_candidate_model(df1['X'],df1['Y'],M)\n",
    "\n",
    "        prediction_jackknife=jackknife_prediction(df1['X'],df1['Y'],M)\n",
    "\n",
    "        Y_train=np.array(df1['Y'],dtype=np.double)\n",
    "\n",
    "        ##################################First: compare their test errors--------------------------------------------\n",
    "        ##JMA\n",
    "        JMA_weights=weights_criterion_jackknife(Y_train=Y_train,prediction_jackknife=prediction_jackknife,\n",
    "                                                kernel_weights=None,candidate_interval_lengths=None,penalty_coef=None,criterion='ordinary')\n",
    "        JMA_predictions=predict_basemodel(M_candidate_models,JMA_weights,df3['X'])\n",
    "\n",
    "        ##weighted_JMA\n",
    "        weighted_JMA_predictions=np.zeros(n)\n",
    "        for i in range(n):\n",
    "            kernel_weights=find_kernel_weights(X=df1['X'],x_datapoint=df3['X'][i,],h=h)\n",
    "            weighted_JMA_weights=weights_criterion_jackknife(Y_train=Y_train,prediction_jackknife=prediction_jackknife,\n",
    "                                                             kernel_weights=kernel_weights,candidate_interval_lengths=None,penalty_coef=None,criterion='weighted')\n",
    "            weighted_JMA_predictions[i]=predict_basemodel(M_candidate_models,weighted_JMA_weights,df3['X'][i,:])   \n",
    "\n",
    "        ###compute their errors\n",
    "        JMA_risk[k]=compute_PP_metric(predictions=JMA_predictions,Y_test=df3['Y'])\n",
    "        weighted_JMA_risk[k]=compute_PP_metric(predictions=weighted_JMA_predictions,Y_test=df3['Y'])\n",
    "\n",
    "\n",
    "\n",
    "        ##################################Second: compare their coverage and interval length--------------------------------------------\n",
    "        SCP_MA_pred_intervals=SCP_MA(M_candidate_models,prediction_jackknife,df2['Y'],df2['X'],df2['Y'],df3['X'],alpha)\n",
    "\n",
    "        SLCP_MA_pred_intervals=SLCP_MA(M_candidate_models,prediction_jackknife,df1['X'],df1['Y'],df2['X'],df2['Y'],df3['X'],h,penalty_coef,alpha)\n",
    "\n",
    "\n",
    "        #####\n",
    "        SCP_MA_metrics=compute_PI_metrics(SCP_MA_pred_intervals,df3['Y'])\n",
    "        SLCP_MA_metrics=compute_PI_metrics(SLCP_MA_pred_intervals,df3['Y'])\n",
    "\n",
    "        ###\n",
    "        SCP_MA_cov[k]=SCP_MA_metrics['cov']\n",
    "        SCP_MA_len[k]=SCP_MA_metrics['len']\n",
    "        SLCP_MA_cov[k]=SLCP_MA_metrics['cov']\n",
    "        SLCP_MA_len[k]=SLCP_MA_metrics['len']\n",
    "    \n",
    "    ######\n",
    "    JMA_risk_vec[t]=np.mean(JMA_risk)\n",
    "    weighted_JMA_risk_vec[t]=np.mean(weighted_JMA_risk)\n",
    "    SCP_cov_vec[t]=np.mean(SCP_MA_cov)\n",
    "    SCP_len_vec[t]=np.mean(SCP_MA_len)\n",
    "    SLCP_cov_vec[t]=np.mean(SLCP_MA_cov)\n",
    "    SLCP_len_vec[t]=np.mean(SLCP_MA_len)\n",
    "    print(t)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5213cc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JMA risk:  [0.826 0.843 0.814 0.842 0.858 0.866 0.883 0.927 1.131]\n",
      "weighted JMA risk:  [0.841 0.859 0.827 0.871 0.922 0.899 0.956 0.996 1.273]\n",
      "SCP coverage:  [0.901 0.891 0.913 0.902 0.908 0.914 0.912 0.907 0.905]\n",
      "SLCP coverage:  [0.899 0.881 0.889 0.898 0.9   0.881 0.886 0.884 0.875]\n",
      "SCP interval length:  [3.399 3.598 3.744 3.826 4.163 4.541 4.937 5.752 8.277]\n",
      "SLCP interval length:  [3.815 3.801 3.825 4.02  3.942 3.891 4.035 4.2   4.975]\n"
     ]
    }
   ],
   "source": [
    "print(\"JMA risk: \", np.round(JMA_risk_vec,3))\n",
    "print(\"weighted JMA risk: \", np.round(weighted_JMA_risk_vec,3))\n",
    "print(\"SCP coverage: \", np.round(SCP_cov_vec,3))\n",
    "print(\"SLCP coverage: \", np.round(SLCP_cov_vec,3))\n",
    "print(\"SCP interval length: \", np.round(SCP_len_vec,3))\n",
    "print(\"SLCP interval length: \", np.round(SLCP_len_vec,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce4c445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码已运行完毕\n"
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "\n",
    "print(\"代码已运行完毕\")\n",
    "\n",
    "# 播放系统提示音\n",
    "winsound.Beep(440, 2000)  # 第一个参数是频率（以赫兹为单位），第二个参数是持续时间（以毫秒为单位）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6842f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
